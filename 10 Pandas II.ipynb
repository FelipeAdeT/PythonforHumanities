{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation Reminder\n",
    "\n",
    "- **Grey cells** are **code cells**. Click inside them and type to edit.\n",
    "- **Run**  code cells by pressing $ \\triangleright $  in the toolbar above, or press ``` shift + enter```.\n",
    "-  **Stop** a running process by clicking $\\Box$ in the toolbar above.\n",
    "- You can **add new cells** by clicking to the left of a cell and pressing ```A``` (for above), or```B``` (for below). \n",
    "- **Delete cells** by pressing```X```.\n",
    "- Run all code cells that import objects (such as the one below) to ensure that you can follow exercises and examples.\n",
    "- Feel free to edit and experiment - you will not corrupt the original files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas II: Making the Most of Tabular Data\n",
    "\n",
    "In the previous lesson, we learned how to access data contained within a DataFrame. Next, we learn to clean, restructure, combine, analyze and visualize tabular information with Pandas.\n",
    "\n",
    "---\n",
    "Questions and exercises are distributed throughout this lesson. Please run the code cell below to import them before starting the lesson. The code will not produce any visible output, but they will be loaded for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QuestionsPandas2 import Q3, question, solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson Goals\n",
    "\n",
    "- Sort, add and remove columns and rows\n",
    "- Perform math on columns and rows\n",
    "- Understand Tidy Data and pivot using **df.melt()**\n",
    "- Join and append tables through **df.merge()** and **df.append()**\n",
    "- Understand basics of **visualizing dataframes** in Pandas\n",
    "\n",
    "**Key Concepts:** Tidy data, join, merge, melt\n",
    "\n",
    "---\n",
    "\n",
    "Let's go ahead and import Pandas as pd and create a DataFrame called object_table with the same excel file we used then. The file path is 'Other_files/NGAData.xlsx', and the sheet_name is 'objects'. Try it out for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_table.set_index('objectid',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution(Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Note on Tidy Data\n",
    "\n",
    "**Tidy**, or **tall**, **data** is the preferred format for use with Pandas. If you are familiar with R or Tableau, you should be familiar with tidy data.\n",
    "\n",
    "Datasets contain **values**, **variables** and **observations**.\n",
    "\n",
    ">A dataset is a collection of **values**, usually either numbers (if quantitative) or strings (if qualitative). Values are organised in two ways. Every value belongs to a variable and an observation. A **variable** contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An **observation** contains all values measured on the same unit (like a person, or a day, or a race) across attributes.\n",
    "\n",
    "**With tidy data, each variable forms one column, each observation forms a row, and each observational unit forms a table.** \n",
    "\n",
    "Often, datasets don't fulfill these requisites, with common problems being that:\n",
    "\n",
    "- Column headers are values, not variable names (i.e., one variable is stored in several columns).\n",
    "- Multiple variables are stored in one column.\n",
    "- Variables are stored in both rows and columns.\n",
    "- Multiple types of observational units are stored in the same table. \n",
    "- A single observational unit is stored in multiple tables.\n",
    "\n",
    "Visually, a dataset like this:\n",
    "\n",
    "| person | treatment | result |\n",
    "|--|--|--|\n",
    "|John|a|NaN|\n",
    "|Jane|a|16|\n",
    "|Mary|a|3|\n",
    "|John|b|2|\n",
    "|Jane|b|11|\n",
    "|Mary|b|1|\n",
    "\n",
    "Is better than one like this:\n",
    "\n",
    "| person | treatment a| treatment b |\n",
    "|--|--|--|\n",
    "|John|NaN|2|\n",
    "|Jane|16|11|\n",
    "|Mary|3|1|\n",
    "\n",
    "That is because the dataset contains 18 values representing 3 variables and 6 observations\n",
    "- person, with three possible values (John, Mary and Jane)\n",
    "- treatment, with two possible values (a and b)\n",
    "- result, with five or six possible values, depending on how you think of the missing value (-, 16, 3, 2, 11, 1)\n",
    "\n",
    "Source:http://vita.had.co.nz/papers/tidy-data.pdf\n",
    "\n",
    "We won't get further into tidy data here, but a great resource are Eric Monson's talks below, and his Github repository on Jupyter and Pandas, which includes two notebooks on Tidy Data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing & Cleaning DataFrames\n",
    "\n",
    "Oftentimes, we will need to edit the columns and rows in a table in different ways. In this section, we will look at sorting dataframes, adding and removing columns, joining datasets and removing duplicate values, as well as simple math functions.  \n",
    "\n",
    "## Sorting Dataframes\n",
    "\n",
    "```df.sort_values()``` is the method for sorting dataframes according to a column's contents. When applying this, we must note that it is not affecting the original dataframe. Thus, we either have to assign our modified dataframe to a variable or specify that we want the method to modify the original values.\n",
    "\n",
    "We should be familiar with assigning variables by now:\n",
    "\n",
    "```python\n",
    "df2= df2.sort_values('column') \n",
    "```\n",
    "\n",
    "In this case, we are reassigning the original dataframe variable to its new value, a common approach.\n",
    "\n",
    "Alternatively, through the in_place keyword, we can modify the dataframe directly.\n",
    "\n",
    "```python\n",
    "df2.sort_values('column', in_place=True) \n",
    "```\n",
    "\n",
    "The default is to sort by ascending order; to change to descending, we must specify ascending=False.\n",
    "\n",
    "In our object table, it might make sense to sort objects by their year of creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_table.sort_values('beginyear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and Removing columns\n",
    "\n",
    "To add a column, just call an unassigned column name with bracket notation and specify the desired value for all cells.\n",
    "\n",
    "```python\n",
    "df['newname'] = x\n",
    "```\n",
    "\n",
    "To remove a column, use the df.drop() method:\n",
    "\n",
    "```python\n",
    "df = df.drop('column_name', 1)\n",
    "```\n",
    "Once again, you have to reassign the variable or specify in_place=True.\n",
    "You must also specify the axis: 1 refers to columns, 0 to rows).\n",
    "\n",
    "\n",
    "**Exercise:** As a test, create a new column titled 'testcolumn' with a blank value ('') and then remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution(Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining and Appending Datasets\n",
    "\n",
    "Often we will find ourselves with two related tables that we want to combine in order to analyze them together. We might have two tables with **different variables for the same observations**; in this case, we can **join** them. Joining two tables results in a table that adds variables (columns) to existing records. \n",
    "\n",
    "> Join: A join combines rows in one or more tables based on common values\n",
    "\n",
    "Alternatively, we might have two tables with the **same variables, and different observations**. In this case, we might want to add rows at the end of one table. For this, we **append** information to a DataFrame.\n",
    "\n",
    "> Appending adds rows with like columns to a table.\n",
    "\n",
    "### Joining Tables\n",
    "\n",
    "We can join two dataframes based on a common key: Python will identify rows in both tables with the same key and output a new table with variables from both tables for that key.\n",
    "\n",
    "There are 4 types of joins. Suppose we have two tables, left and right:\n",
    "\n",
    "|Types of Joins ||\n",
    "--------------|-----|\n",
    "|Left| Keeps all rows in the left table, and will join these to rows in right table with a matching key.|\n",
    "|Inner| Only gives us rows with matching in both tables (will also drop Left-only rows)|\n",
    "|Outer| Returns all rows from both tables, regardless of if they have a match.|\n",
    "|Right| Returns all rows in righ table, joined to any rows with a matching key from the left table.|\n",
    "\n",
    "Be conscientious about the type of join you employ, as it will determine  the table(s) from which you might lose records.\n",
    "\n",
    "The method for joining dataframes is ```df.merge()```, and it is specified in this way:\n",
    "\n",
    "```python\n",
    "df3 = df1.merge(df2, how='left', on='state')\n",
    "```\n",
    "The 'how' keyword specifies the type of join to execute, where df1 acts as the left table, and df2 acts as the right. \n",
    "\n",
    "The keyword 'on' determines the name of the common key. If the names of the key columns are not the same in both tables, you can use left_on and right_on to specify.\n",
    "\n",
    "Let's import some information to join to our objects_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_table = pd.read_csv('Other_Files/NGAData/constituents.csv')\n",
    "join_table = pd.read_csv('Other_Files/NGAData/objects_constituents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tables are from a relational database, and objects_table and people_table are intended to be merged with the join_table. The join table consists of rows with  identifiers from both tables, as well as some attributes that describe the relationship.\n",
    "\n",
    "So to relate 'objects' to 'people', we will have to realize two joins. Let's run the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join1 = objects_table.merge(join_table, how='left', on='objectid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see if we look at the columns of the joined dataframe, rows now have incorporated information from the second table ('constituentid', 'displayorder','roletype', etc.) You might suspect what the key that could link this table to our people table might be (hint: the CSV that held the table was called 'constituents').\n",
    "\n",
    "**Exercise 3:** Edit the code cell below to perform the second join by linking our first joined table (join1) to the people_table using a 'left' join and the key called 'constituentid'. \n",
    "\n",
    "If you want to check your answer in a new cell by calling join2, note that the object information should appear first, followed by the relationship, then the person (otherwise, you might have switched the order of the tables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join2 = ____._____(______,how='____', on='______')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution(Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we make selections from the new table, we can make selections that relate the contents of both tables.\n",
    "\n",
    "For instance, we can check the museum's holdings of works by the artist with ID 1400:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join2[join2['constituentid']==1400][['title','preferreddisplayname']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending Datasets\n",
    "\n",
    "To append datasets together, that is, to add one table as new rows in another, you need two tables with the same column structure.\n",
    "\n",
    "Then, you use the ```df.append()``` method:\n",
    "\n",
    "```python\n",
    "df1.append(df2)\n",
    "```\n",
    "\n",
    "This method also accepts dictionaries and other series, in lieu of a second dataframe. For more information, consult the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicate values\n",
    "\n",
    "Another useful method, available for dataframes or series, is the option of finding only unique values.\n",
    "\n",
    "For the full dataframe, this is is done with the df.drop_values() method.\n",
    "\n",
    "```python\n",
    "df2 = df.drop_values('columnname')\n",
    "```\n",
    "This will return a selection of rows without repeating the values in the column.\n",
    "\n",
    "The series.unique() method will return a list of unique values within a single column:\n",
    "\n",
    "```python\n",
    "df['column_name'].unique()\n",
    "```\n",
    "\n",
    "For instance, we can generate a list of artists represented in the NGA collection, and count them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = people_table[people_table['artistofngaobject']==1]['preferreddisplayname']\n",
    "artists = artists.unique()\n",
    "artists = artists.tolist()\n",
    "len(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math on DataFrames\n",
    "\n",
    "Series and DataFrames have mathematical methods associated to them like ```sum()```,```mean()```, ```median()```,```max()```, ```min()```...\n",
    "\n",
    "In Pandas, math is applied efficiently to columns or rows. It is applied down columns by default. To act upon rows, you must specify otherwise by including axis=1 inside the parentheses.\n",
    "\n",
    "Strings are ignored or handled in a logical way. NaN/Null values are ignored by default, instead of causing errors.\n",
    "\n",
    "Dataframes have mathematical methods: \n",
    "\n",
    "```python\n",
    "df.sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(objects_table.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As do single columns, because they are series.\n",
    "\n",
    "```\n",
    "df['column'].sum()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(objects_table['beginyear'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting the above two examples to be legible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use operators to act upon values in columns. For instance:\n",
    "    \n",
    "```python\n",
    "df['newcolumn'] = df['column1'] + df['column2']\n",
    "```\n",
    "Which returns a series of answers (adds cells in same row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transpose()\n",
    "df.to_dict()\n",
    "df.to_numpy()\n",
    "df.to_json()\n",
    "df.to_csv()\n",
    "\n",
    "groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Plotting with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions\n",
    "\n",
    "Dataframes also come with the plot() method, which allows us to create different types of graphs. \n",
    "\n",
    "There are two ways to access the plot method, either including the graph type in parenthesis or including it with a dot:\n",
    "\n",
    "```python\n",
    "df.plot(kind='line', x='column', y=['column1', 'column2'])\n",
    "df.plot.line(x='column',y=['column1', 'column2'])\n",
    "```\n",
    "\n",
    "Plotting in Pandas is like plotting in excel: one column will be X values, all others will be Y. (Wide format, vs Tableau, Altair, Seaborn which require tall/tidy data: year in one column, field in another and one type of data per column. \n",
    "\n",
    "If we specify X in pandas, it will by default take all other columns for Y.\n",
    "\n",
    "; gets rid of text output before plot.\n",
    "\n",
    "\n",
    "# Bar Chart\n",
    "\n",
    "df.mean() (of each column)\n",
    "\n",
    "df.mean().plot.bar() vertical\n",
    "df.mean().plot.barh() horizontal\n",
    "\n",
    "Sorted bar charts\n",
    "vertical bar charts go left to right, horizontal from down to up\n",
    "\n",
    "df.mean().sort_values(ascending=False).plot.bar();\n",
    "\n",
    "df.mean().sort_values().plot.barh();\n",
    "\n",
    "# Histogram\n",
    "\n",
    "df.hist() gives you a grid of all your different columns and a histogram of each of those. 10 bins by default. Shows you all range by default, not consistent x-y values through graphs. There is an argument for same axis on all\n",
    "\n",
    "df.hist(sharex=True, sharey=True, layout(2,3), figsize=(10,5));\n",
    "\n",
    "Another way to do it is\n",
    "df.plot.hist()\n",
    "Gives you something different: one set of axes and bins, different colors for multiple columns. you can set the alpha value down to make transparent. \n",
    "\n",
    "# Box Plot\n",
    "\n",
    "df.plot.box()\n",
    "\n",
    "Sort works the same, but took median separately, sorted it and then used their index to create a sorted list of column names.\n",
    "\n",
    "horizontal box plot\n",
    "df.plot.box(vert=false) -- different from bar charts\n",
    "\n",
    "Syntax of plots are all slightly different, so powerful but there are better modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lesson Summary\n",
    "\n",
    "<div style=\"text-align:center\">    \n",
    "  <a href=\"09%20Pandas.ipynb\">Previous Lesson: Pandas</a>|\n",
    "   <a href=\"11%20Power%20Up%20Your%20Python.ipynb\">Next Lesson: Power up your Python</a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
