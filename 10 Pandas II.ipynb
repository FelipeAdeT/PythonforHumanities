{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation Reminder\n",
    "\n",
    "- **Grey cells** are **code cells**. Click inside them and type to edit.\n",
    "- **Run**  code cells by pressing $ \\triangleright $  in the toolbar above, or press ``` shift + enter```.\n",
    "-  **Stop** a running process by clicking &#9634; in the toolbar above.\n",
    "- You can **add new cells** by clicking to the left of a cell and pressing ```A``` (for above), or ```B``` (for below). \n",
    "- **Delete cells** by pressing ```X```.\n",
    "- Run all code cells that import objects (such as the one below) to ensure that you can follow exercises and examples.\n",
    "- Feel free to edit and experiment - you will not corrupt the original files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas II: Making the Most of Tabular Data\n",
    "\n",
    "In the previous lesson, we learned how to access data contained within a DataFrame. Next, we learn to clean, restructure, combine, analyze and visualize tabular information with Pandas.\n",
    "\n",
    "---\n",
    "Questions and exercises are distributed throughout this lesson. Please run the code cell below to import them before starting the lesson. The code will not produce any visible output, but exercises and questions will be loaded for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QuestionsPandas2 import E1, E2, E3, E4, E5, question, solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson Goals\n",
    "\n",
    "- Sort, add and remove columns and rows\n",
    "- Perform math on columns and rows\n",
    "- Understand Tidy Data and pivot using **df.melt()**\n",
    "- Join and append tables through **df.merge()** and **df.append()**\n",
    "- Understand basics of **visualizing DataFrames** in Pandas\n",
    "\n",
    "**Key Concepts:** Tidy data, join, merge, melt\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 1:** Using what you learned in previous lessons, go ahead and import Pandas as pd in the cell below. Afterwards, create a DataFrame called 'objects' with the excel file 'NGADataSample.xlsx. The file path is 'Other_files/NGADataSample.xlsx', and the sheet_name is 'objects'. Try it out for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects.set_index('objectid',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution(E1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, examine the object in one of the ways discussed previously in the course. You can call its name, use the df.head() or df.tail() method, or the df.columns attribute. It should have 20 rows and 6 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing & Cleaning DataFrames\n",
    "\n",
    "Oftentimes, we will need to modify the columns and rows in a table in different ways. In this section, we will look at sorting dataframes, adding and removing columns, joining datasets and removing duplicate values.  \n",
    "\n",
    "## Sorting Dataframes\n",
    "\n",
    "```df.sort_values()``` is the method for sorting dataframes according to a column's contents. When applying this, we must note that it is not affecting the original dataframe. Thus, we either have to assign our modified dataframe to a variable or specify that we want the method to modify the original values \"in place.\"\n",
    "\n",
    "We should be familiar with assigning variables by now:\n",
    "\n",
    "```python\n",
    "df2 = df2.sort_values('column') \n",
    "```\n",
    "\n",
    "In this case, we are reassigning the original dataframe variable to its new value, a common approach.\n",
    "\n",
    "Alternatively, through the in_place keyword, we can modify the dataframe directly.\n",
    "\n",
    "```python\n",
    "df2.sort_values('column', in_place=True) \n",
    "```\n",
    "\n",
    "The default is to sort by ascending order; to change to descending, we must specify ascending=False.\n",
    "\n",
    "In our objects DataFrame, it might make sense to sort objects by their year of creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects.sort_values('beginyear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and Removing Columns and Rows\n",
    "\n",
    "To add a column, just call an unassigned column name with bracket notation and specify the desired value for all cells.\n",
    "\n",
    "```python\n",
    "df['newname'] = x\n",
    "```\n",
    "\n",
    "To remove a column, use the df.drop() method:\n",
    "\n",
    "```python\n",
    "df = df.drop(columns = 'column_name')\n",
    "```\n",
    "Once again, you have to reassign the variable or specify in_place=True. You can include multiple columns by providing a list. \n",
    "\n",
    "The same method applies for dropping rows, where instead of the keyword argument 'columns', you use the keyword 'index' and specify the indices of the rows you would like to remove.\n",
    "\n",
    "```python\n",
    "df = df.drop(index = [index numbers])\n",
    "```\n",
    "\n",
    "**Exercise 2:** As a test, using the cells below, create a new column titled 'testcolumn' with a blank value ('') and then remove it. You can check the changes you make by calling the dataframe or the df.columns attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution(E2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Tidy Data\n",
    "\n",
    "**Tidy**, or **tall**, **data** is the preferred format for many types of data analysis and visualization, including Pandas and multiple Python modules. If you are familiar with R or Tableau, you should be familiar with tidy data. \n",
    "\n",
    "Datasets contain **values**, **variables** and **observations**.\n",
    "\n",
    ">A dataset is a collection of **values**, usually either numbers (if quantitative) or strings (if qualitative). Values are organised in two ways. Every value belongs to a variable and an observation. A **variable** contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An **observation** contains all values measured on the same unit (like a person, or a day) across attributes.\n",
    "\n",
    "**With tidy data, each variable forms one column, each observation forms a row, and each observational unit forms a table.** \n",
    "\n",
    "Often, datasets don't fulfill these requisites, with common problems being that:\n",
    "\n",
    "- Column headers are values, not variable names (i.e., one variable is stored in several columns).\n",
    "- Multiple variables are stored in one column.\n",
    "- Variables are stored in both rows and columns.\n",
    "- Multiple types of observational units are stored in the same table. \n",
    "- A single observational unit is stored in multiple tables.\n",
    "\n",
    "Analytically, a dataset like this:\n",
    "\n",
    "| person | treatment | result |\n",
    "|--|--|--|\n",
    "|John|a|NaN|\n",
    "|Jane|a|16|\n",
    "|Mary|a|3|\n",
    "|John|b|2|\n",
    "|Jane|b|11|\n",
    "|Mary|b|1|\n",
    "\n",
    "Is better than one like this:\n",
    "\n",
    "| person | treatment a| treatment b |\n",
    "|--|--|--|\n",
    "|John|NaN|2|\n",
    "|Jane|16|11|\n",
    "|Mary|3|1|\n",
    "\n",
    "That is because the dataset contains 18 values representing 3 variables and 6 observations\n",
    "- person, with three possible values (John, Mary and Jane)\n",
    "- treatment, with two possible values (a and b)\n",
    "- result, with five or six possible values, depending on how you think of the missing value (-, 16, 3, 2, 11, 1)\n",
    "\n",
    "> Source:http://vita.had.co.nz/papers/tidy-data.pdf\n",
    "\n",
    "We will often find untidy data structures in data that is intended for display, such as summary tables.\n",
    "\n",
    "We won't get further into tidy data here, but a great resource are Eric Monson's talks below, and his Github repository on Jupyter and Pandas, which includes two notebooks on Tidy Data.\n",
    "\n",
    "A very useful method for achieving a clean(er) dataset is the **``` df.melt()```** method. This method will unpivot columns into rows, thus helping reorganize values that have been erroneously included as variables.\n",
    "\n",
    "```python\n",
    "df.melt(id_vars = [list of variables to remain unchanged], value_vars = [list of variables to be made into values], var_name = 'nameforvariable', value_name = 'nameforvalues')\n",
    "```\n",
    "\n",
    "To clarify, in our example, the melt would look something like this:\n",
    "\n",
    "```python\n",
    "treatment_table = treatment_table.melt( id_vars = 'person', value_vars = ['treatment a', 'treatment b'], var_name = 'treatment', value_name = 'result')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_table = pd.DataFrame({'person':{0: 'John', 1: 'Jane',2: 'Mary'}, 'treatment a': {0: None, 1: 16, 2: 3}, 'treatment b': {0:2, 1:11,2:1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_table.melt(id_vars = 'person', value_vars = ['treatment a', 'treatment b'], var_name = 'treatment', value_name = 'result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Say we have a table we need to format for analysis with Python or Tableau. The dataset below gives historical information on several ghettoes in the Krakow district in Poland. Restructure the data to include an 'Epidemic' variable with values 'UnspecE', 'UnknownE', 'UncertE', 'TyphusE', 'TyphoidE', 'TuberE', 'no_E', 'DysenteryE'. The values of these columns can be called 'EpidemicYN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghettoes = pd.read_csv('Other_files/KrakowGhettoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution(E3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining or Merging Datasets\n",
    "\n",
    "Often we will find ourselves with two related tables that we want to combine in order to analyze them together. We might have two tables with **different variables for the same observations**. Joining (as it is known in SQL) or **merging** (as it is referred to in Pandas) is the action of combining two datasets that give different variables for the same observations. Joining two tables results in a table that adds variables (columns) to existing records. \n",
    "\n",
    "> Join: A join combines rows in one or more tables based on common values\n",
    "\n",
    "We can **join** or **merge** two dataframes based on a common unique identifier (also known as a **key**): Python will identify rows in both tables with the same key and output a new table with variables from both tables for that key.\n",
    "\n",
    "For two tables, left and right), there are 4 types of joins. These vary according to the priority given to the either table and according to the treatment of non-matching values. Be conscientious about the type of join you employ, as the choice will determine the table(s) from which non-matching values will be dropped.\n",
    "\n",
    "Suppose we have two tables, left and right:\n",
    "\n",
    "|Types of Joins ||\n",
    "--------------|-----|\n",
    "|Left| Keeps all rows in the left table, and will join these to rows in right table with a matching key.|\n",
    "|Inner| Only gives us rows with matching in both tables (will also drop Left-only rows)|\n",
    "|Outer| Returns all rows from both tables, regardless of if they have a match.|\n",
    "|Right| Returns all rows in righ table, joined to any rows with a matching key from the left table.|\n",
    "\n",
    "\n",
    "\n",
    "The method for joining dataframes is ```df.merge()```, and it is specified in this way:\n",
    "\n",
    "```python\n",
    "df3 = df1.merge(df2, how='left', on='key_column_name')\n",
    "```\n",
    "The 'how' keyword specifies the type of join to execute, where df1 acts as the left table, and df2 acts as the right. \n",
    "\n",
    "The keyword 'on' determines the name of the common key. If the names of the key columns are not the same in both tables, you can use left_on and right_on to specify.\n",
    "\n",
    "Let's import some information to join to our objects DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.read_excel('Other_files/NGADataSample.xlsx', sheet_name='people')\n",
    "join_table = pd.read_excel('Other_files/NGADataSample.xlsx', sheet_name = 'jointable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tables are from a relational database, and objects_table and people_table are intended to be merged with the join_table. The join table consists of rows with  identifiers from both tables, as well as some attributes that describe the relationship.\n",
    "\n",
    "So to relate 'objects' to 'people', we will have to realize two joins. Let's run the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join1 = objects.merge(join_table, how='left', on='objectid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see if we look at the columns of the joined dataframe, rows now have incorporated information from the second table ('constituentid', 'displayorder','roletype', etc.) You might suspect what the key that could link this table to our people table might be (hint: the CSV that held the table was called 'constituents')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(join1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also notice that the join table is substantially longer than the original objects table. This is because there was a one-to-many relationship between rows in the left and right tables. One object row had multiple people associated to it in the join table (perhaps owners, donors or artists, etc).\n",
    "\n",
    "**Exercise 4:** Edit the code cell below to perform the second join by linking our first joined table (join1) to the people_table using a 'left' join and the key called 'constituentid'. \n",
    "\n",
    "If you want to check your answer in a new cell by calling join2, note that the object information should appear first, followed by the relationship, then the person (otherwise, you might have switched the order of the tables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join2 = ____._____(______,how='____', on='______')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution(E4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join2 = join1.merge(people,how='left', on='constituentid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we make selections from the new table, we can make selections that relate the contents of both tables.\n",
    "\n",
    "For instance, we can check the works of art by the artist with ID 2136:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join2.loc[(join2['constituentid']==2136),['title','preferreddisplayname']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending Datasets\n",
    "\n",
    "Alternatively, we might have two tables with the **same variables, and different observations**. In this case, we might want to add rows at the end of one table. For this, we **append**  or **concatenate** information to a DataFrame.\n",
    "\n",
    "> **Appending** adds rows with like variables to a table.\n",
    "\n",
    "To append two datasets together, that is, to add one table as new rows in another, you need two tables with the same column structure.\n",
    "\n",
    "Then, you use the ```df.append()``` method:\n",
    "\n",
    "```python\n",
    "df1.append(df2)\n",
    "```\n",
    "\n",
    "This method also accepts dictionaries and other series, in lieu of a second dataframe.\n",
    "\n",
    "The 'ignore_index' argument can also be useful for ignoring the indices from the individual DataFrames. For more information, consult the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html).\n",
    "\n",
    "If you have more than two DataFrames, a more general option is the ```pd.concat()```method. \n",
    "\n",
    "```python\n",
    "dfslist =[DataFrame1, DataFrame2, Dataframe3]\n",
    "\n",
    "newdf = pd.concat(dfslist)\n",
    "```\n",
    "\n",
    "**Exercise 5** The spreadsheet 'moreobjects' in the file 'NGADatSample.xlsx' contains more rows of objects with the same columns as our objects dataframe. In the cells below, import the sheet (filepath: 'Other_files/NGADataSample.xlsx'). Use any name for the new dataframe, but remember not to overwrite our original 'objects' DataFrame.  Use append() or concat() to append these rows to our objectsdataframe.  The new object should have 32 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution(E5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Unique Values\n",
    "\n",
    "Another useful method, available for dataframes or series, is the option of finding only unique values.\n",
    "\n",
    "The series.unique() method will return a list of unique values within a single column:\n",
    "\n",
    "```python\n",
    "df['column_name'].unique()\n",
    "```\n",
    "\n",
    "For instance, we can generate a  unique series of artists in our people DataFrame, transform that into a list, and then count them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = people[people['artistofngaobject']==1]['preferreddisplayname']\n",
    "artists = artists.unique()\n",
    "artists = artists.tolist()\n",
    "len(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **```df.value_counts()```** method accomplishes something similar, but outputs a series with the unique values and gives the number of times that value appears in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.loc[(people['artistofngaobject']==1),'preferreddisplayname'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math on DataFrames\n",
    "\n",
    "Series and DataFrames have mathematical methods associated to them like ```sum()```,```mean()```, ```median()```,```max()```, ```min()```...\n",
    "\n",
    "In Pandas, math is applied efficiently to columns or rows. It is applied down columns by default. To act upon rows, you must specify otherwise by including axis=1 inside the parentheses.\n",
    "\n",
    "Strings are ignored or handled in a logical way. NaN/Null values are ignored by default, instead of causing errors.\n",
    "\n",
    "Dataframes have mathematical methods: \n",
    "\n",
    "```python\n",
    "df.sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(objects_table.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As do single columns, because they are series.\n",
    "\n",
    "```\n",
    "df['column'].sum()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(objects_table['beginyear'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting the above two examples to be legible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use operators to act upon values in columns. For instance:\n",
    "    \n",
    "```python\n",
    "df['newcolumn'] = df['column1'] + df['column2']\n",
    "```\n",
    "Which returns a series of answers (adds cells in same row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transpose()\n",
    "df.to_dict()\n",
    "df.to_numpy()\n",
    "df.to_json()\n",
    "df.to_csv()\n",
    "\n",
    "groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Plotting with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions\n",
    "\n",
    "Dataframes also come with the plot() method, which allows us to create different types of graphs. \n",
    "\n",
    "There are two ways to access the plot method, either including the graph type in parenthesis or including it with a dot:\n",
    "\n",
    "```python\n",
    "df.plot(kind='line', x='column', y=['column1', 'column2'])\n",
    "df.plot.line(x='column',y=['column1', 'column2'])\n",
    "```\n",
    "\n",
    "Plotting in Pandas is like plotting in excel: one column will be X values, all others will be Y. (Wide format, vs Tableau, Altair, Seaborn which require tall/tidy data: year in one column, field in another and one type of data per column. \n",
    "\n",
    "If we specify X in pandas, it will by default take all other columns for Y.\n",
    "\n",
    "; gets rid of text output before plot.\n",
    "\n",
    "\n",
    "# Bar Chart\n",
    "\n",
    "df.mean() (of each column)\n",
    "\n",
    "df.mean().plot.bar() vertical\n",
    "df.mean().plot.barh() horizontal\n",
    "\n",
    "Sorted bar charts\n",
    "vertical bar charts go left to right, horizontal from down to up\n",
    "\n",
    "df.mean().sort_values(ascending=False).plot.bar();\n",
    "\n",
    "df.mean().sort_values().plot.barh();\n",
    "\n",
    "# Histogram\n",
    "\n",
    "df.hist() gives you a grid of all your different columns and a histogram of each of those. 10 bins by default. Shows you all range by default, not consistent x-y values through graphs. There is an argument for same axis on all\n",
    "\n",
    "df.hist(sharex=True, sharey=True, layout(2,3), figsize=(10,5));\n",
    "\n",
    "Another way to do it is\n",
    "df.plot.hist()\n",
    "Gives you something different: one set of axes and bins, different colors for multiple columns. you can set the alpha value down to make transparent. \n",
    "\n",
    "# Box Plot\n",
    "\n",
    "df.plot.box()\n",
    "\n",
    "Sort works the same, but took median separately, sorted it and then used their index to create a sorted list of column names.\n",
    "\n",
    "horizontal box plot\n",
    "df.plot.box(vert=false) -- different from bar charts\n",
    "\n",
    "Syntax of plots are all slightly different, so powerful but there are better modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lesson Summary\n",
    "\n",
    "<div style=\"text-align:center\">    \n",
    "  <a href=\"09%20Pandas.ipynb\">Previous Lesson: Pandas</a>|\n",
    "   <a href=\"11%20Power%20Up%20Your%20Python.ipynb\">Next Lesson: Power up your Python</a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
