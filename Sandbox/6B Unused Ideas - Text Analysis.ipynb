{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequencies: Extending the loop to all our stories\n",
    "\n",
    "The loop above works well for one story, but we have 201 stories to analyze. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "By now, we have the raw data in a format that allows us to begin our analysis. \n",
    "\n",
    "We will be creating a metric to evaluate similarity between documents. For this, we need to evaluate term frequencies, which we can do by counting terms within each document and dividing by the document's length. Once we have this information, we will evaluate document similarity by creating a formula that compares term frequencies in two documents. \n",
    "\n",
    "---\n",
    "# 2. Solving the Problem: Calculating Term Frequencies\n",
    "\n",
    "We should think about the nature of our problem. \n",
    "\n",
    "We have a data structure which is a list in which each item is a list of tokens. For each list of tokens, we will have to identify an individual term, and then count the number of times it appears in the list. Because texts vary in length, we should divide the count by the number of words in the text- the result of this operation is called the **term frequency**. \n",
    "\n",
    "In other words, we will have a number of **unique terms** and an associated frequency **value** for each. This should ring some alarm bells about the best data structure for storing the information. What data structure would you use?\n",
    "\n",
    "Ideally, we would want to create dictionaries of word counts. In each dictionary, each term would be a key, and its count would be the value. There would be one dictionary per story. \n",
    "\n",
    "## Calculating Term Frequencies: A loop for one story\n",
    "\n",
    "What we have to design now is a loop that can generate the count for each word. We can work it out for one poem, before creating a more general loop that works through the 201 stories.\n",
    "\n",
    "First, assign the first poem in our list to the variable 'poem1'. Remember that in a list, the first position is 0. We will use this poem to think about how to work through an individual case before generalizing and applying the method to other poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story1 =stories_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(story1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within our problem, we have to think of two scenarios:\n",
    "\n",
    "For each word in our poem,\n",
    "1. If we are encountering a new word, that is, if the word is not in our data structure, create an item where the key is the token, and the value is 1. \n",
    "2. If the word already has an item in our data structure, retrieve the value and update it by adding 1 to the count.\n",
    "\n",
    "Finally, we have to divide each final count by the number of words in the text.\n",
    "\n",
    "Remember how to access, create and modify information in a dictionary. To retrieve a value from a dictionary, you call the dictionary name and the key in brackets. This method also works to create a new dictionary item, or to update an item's value. Also remember that you can iterate through the keys in a dictionary as you would iterate through the items in a list.\n",
    "\n",
    "\n",
    "Now, write some code that:\n",
    "1. Creates an empty dictionary called term_counts\n",
    "2. Loops through the terms in story 1 and populates the term_counts dictionary with unique terms and the number of times each one appears in the poem. Within the dictionary, each item will be comprised of a key (the token) and a value (the count). Take into account the two situations that could emerge, as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts={}\n",
    "for token in story1:\n",
    "    if token not in term_counts:\n",
    "        term_counts[token]=1\n",
    "    else:\n",
    "        term_counts[token]=term_counts[token]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary above holds the absolute count of terms in story1, but recall that we want the relative frequency of terms. You can calculate this by dividing each count by the length of the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(story1)\n",
    "term_freqs={}\n",
    "for token in term_counts:\n",
    "    term_freqs[token]= term_counts[token]/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequencies: Extending the loop to all our stories\n",
    "\n",
    "The loop above works well for one story, but we have 201 stories to analyze. \n",
    "\n",
    "That means we have to create 201 dictionaries, and store them in a way that is easy to retrieve. Once again, because all we are using is position of the story to identify it, a list (which maintains and is indexable by position) is the best data structure for this case. \n",
    "\n",
    "Create an empty list called stories_freqs.  Then, construct a nested for-loop. First, the loop should iterate through each item in our stories_tokens_lemma list.  Within the loop, for each list of poem tokens, the loop should generate a dictionary of word counts. In other words, the loops you created above can be modified and nested into the new loop to work on all our poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_freqs = []\n",
    "\n",
    "for story in stories_tokens:\n",
    "    term_counts={}\n",
    "    length=len(story)\n",
    "    for token in story:\n",
    "        if token not in term_counts:\n",
    "            term_counts[token]=1\n",
    "        else:\n",
    "            term_counts[token]=term_counts[token]+1\n",
    "    term_freq={}\n",
    "    for term in term_counts:\n",
    "        term_freq[term]=term_counts[term]/length\n",
    "    stories_freqs.append(term_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counting the frequency of terms used in each individual story and then creating a simple algorithm that measures similarity between stories using this information. In the future, such algorithms can be imported from modules created by others; but creating one is a useful way to reinforce the knowledge you obtained in the past lessons, practice working through a small project and avoid getting into the habit of 'outsourcing' algorithms without thinking about our needs and objectives, as well as what these algorithms are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# A simple Algorithm for Document Similarity\n",
    "\n",
    "In the code above, we cleaned the text, split it into tokens, and calculated the term frequencies for all poems. Next, we can use the term frequencies to calculate the similarity between pairs of stories.\n",
    "\n",
    "We can use a simple measure of distance for this purpose. \n",
    "\n",
    ">**Euclidean distance** is the straight-line distance between two points in space.\n",
    "\n",
    "We could think of each document as a point whose location in space is determined by its token frequencies. To simplify the issue, if we had two tokens x and y, their counts would correspond to the point's (x,y) locations in a two-dimensional space. If we had two documents, with (x,y) values (1,2) and 2,3) respectively, for instance, their distance is defined by this formula: \n",
    "\n",
    "$$\\sqrt{(x1-x2)^2+(y1-y2)^2}$$ \n",
    "\n",
    "Conceptually, we are adding the distance in x and the distance in y, where taking the square ensures that we get no errors from negative differences. This formula can be extended through any number of dimensions by adding the square difference for as many terms as necessary. With n tokens, we are thinking of the documents as being positioned in an n-dimensional space, and will have n square differences in our formula.\n",
    "\n",
    "To refine the analysis a bit further, we will also give these terms different weights. While term frequency alone can be informative, many words with little inherent meaning (words such as 'the' or 'and') will be over-represented in the documents. Longer documents will disproportionately have such words, and thus appear to be more similar to other long documents based on length alone, not content.\n",
    "\n",
    "One way to account for this issue is Term Frequency-Inverse Document Frequency  (TF-IDF), which controls for this issue by assigning lower weights to tokens that are frequent throughout many texts.\n",
    "\n",
    "Our approach will thus have two parts:\n",
    "\n",
    "1. Calculate the IDF score for all terms in all our poems\n",
    "2. Measure document similarity through euclidean distance of each documents in TF-IDF scores.\n",
    "\n",
    "# Text Frequency - Inverse Document Frequency\n",
    "\n",
    "The TF-IDF score is very simply the term frequency, which we already calculated, multiplied by a weight that gives a higher value to terms that are infrequent in the documents of the corpus. So, in our measure, common words such as \"the\" or \"and\", which increase with story length, will not contribute too much to text similarity. Less common words will have a higher ponderation. \n",
    "\n",
    "The [IDF](https://en.wikipedia.org/wiki/Tf–idf) is a weight that is calculated by dividing the total number of documents by the number of documents that contain a word (and then transforming it using a logarithm). It can be thought of as a measure of the informativeness of a token. \n",
    "\n",
    "We can calculate the IDF scores for all the terms in our stories. Once we have them, we can include them in our distance measure by multiplying distances by this factor to gauge similarity. \n",
    "\n",
    "Create an empty dictionary called tf_idfs. Then, set up a loop that loops through all the stories in our stories_freqs list. For each term in a story, if the term is not in tf_idfs, create an item with the token as the key and value 1. If the term is in tf_idfs, add 1 to the value. This loop should calculate the number of stories the term appears in, storing the data in a dictionary.\n",
    "\n",
    "Then, create a new loop that iterates through each term in tf_idfs and replaces the value with the logarithm of 201 (the total number of stories) divided by the number of stories the term appears in. You can use np.log() to take the logarithm of the division, which should look like: np.log(201/tf_idfs[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfs = {}\n",
    "\n",
    "for story in stories_freqs:\n",
    "    for token in story:\n",
    "        if token not in tf_idfs:\n",
    "            tf_idfs[token]=1\n",
    "        else:\n",
    "            tf_idfs[token]=tf_idfs[token]+1\n",
    "\n",
    "N_stories= len(stories)\n",
    "\n",
    "for token in tf_idfs:\n",
    "    tf_idfs[token]=np.log(N_stories/tf_idfs[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a similarity score between Two Stories\n",
    "\n",
    "Once again, to figure out the general loop, it helps to compare two stories first. \n",
    "\n",
    "Save the first and second items of the stories_freqs list as story1 and story2, respectively. Remembering how indexing works in lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story1 =stories_freqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story2 =stories_freqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a simple algorithm for comparing how similar two sonnets are to one another. Using the TF-IDF score (term counts multiplied by IDF score), we will compare the number of times a token appears in both sonnets.  \n",
    "\n",
    "As we did before with our first loop, we can start thinking of a solution by first looking at two poems.\n",
    "\n",
    "## Euclidean Distance Loop (2 Poems)\n",
    "\n",
    "Recall that the general formula we are going to use is \n",
    "\n",
    "$$\\sqrt{(x1-x2)^2+(y1-y2)^2}$$\n",
    "\n",
    "Where, for two documents 1 and 2,  x1 and x2 represent the frequencies for a term. We will be using the TF-IDF score, that is, the term frequency multipled by the IDF score for our token. \n",
    "\n",
    "That is, to compare two documents, we need:\n",
    "\n",
    "1. For each term, to calculate the square difference by:\n",
    "    1. Multiplying its term frequency in each document by the IDF to get the respective TF-IDF scores. \n",
    "    1. Take the difference of these values\n",
    "    1. Square the result.\n",
    "1. Add the square differences of all terms\n",
    "1. Take the square root of the total sum.\n",
    "\n",
    "Create a script that calculates the Euclidean distance. It helps to create a variable called diffsum with value 0, as well as a set of all the tokens in both your documents using set unions. Then, you can loop through all the tokens in the set, and update the diffsum value as you calculate the square difference for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffsum=0\n",
    "tokens = set(story1)|set(story2) # create a list of all tokens in both poems\n",
    "\n",
    "for token in tokens:\n",
    "    if token in story1:\n",
    "        a=story1[token]*tf_idfs[token]\n",
    "    else:\n",
    "        a=0\n",
    "    if token in story2:\n",
    "        b= story2[token]*tf_idfs[token]\n",
    "    else:\n",
    "        b=0\n",
    "    term = (a-b)**2\n",
    "    termsum= termsum+term\n",
    "    \n",
    "euc_similarity = 1/(1+termsum**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean Distance - Comparing Poem 1 to all other poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = stories_freqs[0]\n",
    "c0_sims = []\n",
    "\n",
    "for c1 in stories_freqs:\n",
    "    tokens = set(c0)|set(c1) # create a list of all tokens in both poems\n",
    "    termsum=0\n",
    "    for token in tokens:\n",
    "        if token in c0:\n",
    "            a=c0[token]*tf_idfs[token]\n",
    "        else:\n",
    "            a=0\n",
    "        if token in c1:\n",
    "            b= c1[token]*tf_idfs[token]\n",
    "        else:\n",
    "            b=0\n",
    "        term = (a-b)**2\n",
    "        termsum= termsum+term\n",
    "    euc_similarity = 1/(1+termsum**0.5)\n",
    "    c0_sims.append(round(euc_similarity,3))\n",
    "print(c0_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(c0_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(c0_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One reason we might want to store this info as a dictionary: \n",
    "# if we sort on value, we lose order and therefore reference to the original poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0_sims.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0_sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Overlapping Words in Two Most Similar Poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eric's Code\n",
    "def overlap_terms(index_a, index_b):\n",
    "    c_a = counts_nonstop_list[index_a]\n",
    "    c_b = counts_nonstop_list[index_b]\n",
    "    only_a = []\n",
    "    only_b = []\n",
    "    both_ab = []\n",
    "    for term,count in c_a.items():\n",
    "        if term in c_b:\n",
    "            both_ab.append(((term,count),(term,c_b[term])))\n",
    "        else:\n",
    "            only_a.append((term,count))\n",
    "    for term,count in c_b.items():\n",
    "        if term not in c_a:\n",
    "            only_b.append((term,count))\n",
    "    # sorting overlapped terms by the sum of the frequencies\n",
    "    both_sorted = sorted(both_ab, key=lambda x:x[0][1]+x[1][1], reverse=True)\n",
    "    a_sorted = sorted(only_a, key=lambda x:x[1], reverse=True)\n",
    "    b_sorted = sorted(only_b, key=lambda x:x[1], reverse=True)\n",
    "    # Just returning the first 15 of each non-overlap list\n",
    "    return [('a','b')] + both_sorted + [('a','b')] + list(zip(a_sorted,b_sorted))[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sets\n",
    "\n",
    "overlap = set(stories_freqs[0].keys()) & set(stories_freqs[59].keys())\n",
    "only_a = set(stories_freqs[0])-set(stories_freqs[59])\n",
    "only_b = set(stories_freqs[59])-set(stories_freqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in overlap:\n",
    "    print(item, stories_freqs[0][item],stories_freqs[59][item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in only_a:\n",
    "    print(item, stories_freqs[0][item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in only_b:\n",
    "    print(item, stories_freqs[59][item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(all_counts):\n",
    "    all_sims_list = []\n",
    "    tokens = set()\n",
    "    for c0 in all_counts:\n",
    "        sims_list = []\n",
    "        for tc in all_counts:\n",
    "            tokens = set(tc)|set(c0)\n",
    "            termsum = 0\n",
    "            for token in tokens:\n",
    "                if token in c0:\n",
    "                    a=c0[token]*tf_idfs[token]\n",
    "                else:\n",
    "                    a=0\n",
    "                if token in tc:\n",
    "                    b= tc[token]*tf_idfs[token]\n",
    "                else:\n",
    "                    b=0\n",
    "                term = (a-b)**2\n",
    "                termsum =termsum+term\n",
    "            euc_similarity = 1/(1+termsum**0.5)\n",
    "            sims_list.append(euc_similarity)\n",
    "        norm_sims_id_list = [round(xx,3) for xx in sims_list]\n",
    "        all_sims_list.append(norm_sims_id_list)\n",
    "    return all_sims_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_sims_list = similarity_matrix(stories_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ns_array = np.array(only_sims_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "ax = sns.heatmap(sim_ns_array, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(sim_ns_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories[163])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories[96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories[139])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
